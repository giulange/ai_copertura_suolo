{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdd3ca1-a421-45b5-bf49-f1be8f5503b8",
   "metadata": {},
   "source": [
    "# Computer Vision & Agriculture / Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a03aef-8017-4145-b989-8621388fe68f",
   "metadata": {},
   "source": [
    "## 1. Tipi di problemi in visione artificiale\n",
    "- **Classificazione:** assegno un‚Äôetichetta a un‚Äôintera immagine o tile (es. questo patch 64√ó64 √® ‚ÄúForest‚Äù).\n",
    "- **Detection:** localizzo oggetti all‚Äôinterno dell‚Äôimmagine con bounding box.\n",
    "- **Segmentazione:** classifico ogni pixel (es. U-Net).\n",
    "- Requisiti strutturali dei modelli:\n",
    "  - Classificazione ‚Üí CNN compatte (Conv+Pooling+FC).\n",
    "  - Detection ‚Üí backbone CNN + testata di localizzazione (es. Faster R-CNN, YOLO).\n",
    "  - Segmentazione ‚Üí architetture encoder‚Äìdecoder con skip connections (es. U-Net).\n",
    "- Perch√© ci concentriamo sulla classificazione CNN: il dataset EuroSAT √® gi√† tile-based, non richiede bounding box n√© maschere per pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51a552-4df9-45ca-98c3-2f760cf1313c",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"artwork/comp-vision-models.webp\" alt=\"Confronto tra classificazione, object detection e instance segmentation\" style=\"max-width:100%; height:auto;\">\n",
    "  <figcaption style=\"font-size:90%; color:#444; margin-top:6px;\">\n",
    "    Figura 1. Confronto tra <b>classificazione</b>, <b>object detection</b> e <b>instance segmentation</b>.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dae9ed-0257-4211-bb83-73c38967c0da",
   "metadata": {},
   "source": [
    "## 2.  CNN vs FFNN\n",
    "- **FFNN:** ogni neurone collegato a tutti i pixel ‚Üí esplosione parametri, nessuna nozione di spazialit√†.\n",
    "- **CNN:** kernel locali + condivisione pesi ‚Üí molto meno parametri, sfruttano la struttura spaziale e spettrale.\n",
    "- Motivazione pratica: per immagini multispettrali, le CNN sono lo strumento ‚Äúnaturale‚Äù per catturare pattern locali e inter-banda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbddd5-9d58-45e7-b950-cc4dedf22d26",
   "metadata": {},
   "source": [
    "## Sintesi dei tre approcci: ANN, CNN, U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c585ce-3ecc-4e6c-8798-6abbb25ab4b4",
   "metadata": {},
   "source": [
    "### 1. Reti Neurali Artificiali (ANN / FFNN)\n",
    "\n",
    "Le **Artificial Neural Networks** (in particolare le **FeedForward Neural Networks**) rappresentano l‚Äôapproccio pi√π semplice e generalista all‚Äôapprendimento automatico.\n",
    "\n",
    "- **Funzionamento**: ogni unit√† di input (es. un valore numerico per ciascuna banda spettrale di un pixel) viene collegata a pi√π livelli di neuroni nascosti, con connessioni completamente connesse (dense).\n",
    "- **Applicazione**: nei dati da telerilevamento, un‚ÄôANN pu√≤ essere utilizzata trattando ogni **pixel come un‚Äôosservazione indipendente**, usando le bande spettrali come caratteristiche.\n",
    "- **Limite**: ignora completamente la **struttura spaziale** dell‚Äôimmagine (cio√® i rapporti tra pixel vicini), e dunque pu√≤ risultare poco efficace per compiti legati all‚Äôanalisi di immagini, dove i pattern spaziali sono cruciali.\n",
    "\n",
    "> ‚úÖ Utile per introdurre i concetti base della classificazione.  \n",
    "> ‚ùå Inadeguato per sfruttare l‚Äôinformazione spaziale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d84c00-12b5-4fb9-9917-09255bda15de",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"artwork/ANN-clay.png\" alt=\"Analisi spaziale 3D delle propriet√† del suolo\" style=\"max-width:100%; height:auto;\">\n",
    "  <figcaption style=\"font-size:90%; color:#444; margin-top:6px;\">\n",
    "    Figura 2. Analisi spaziale 3D delle propriet√† del suolo utilizzando una <b>Rete Neurale Feed-Forward (FFNN) 6:6:6:1</b>.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef69e6-11d6-4538-ba39-4305e9ea1216",
   "metadata": {},
   "source": [
    "### 2. Reti Neurali Convoluzionali (CNN)\n",
    "\n",
    "Le **Convolutional Neural Networks** rappresentano un salto qualitativo, essendo progettate **specificamente per lavorare con immagini**.\n",
    "\n",
    "- **Funzionamento**: usano filtri (kernel) che ‚Äúscandagliano‚Äù l‚Äôimmagine estraendo pattern locali ricorrenti, come contorni, forme e texture.\n",
    "- **Applicazione**: permettono di **classificare un‚Äôimmagine intera** oppure associare una **classe dominante** a un‚Äôimmagine o a una finestra (patch).\n",
    "- **Vantaggio**: tengono conto del **contesto spaziale** e della **localizzazione** dei pattern.\n",
    "\n",
    "> ‚úÖ Ideali per compiti di **classificazione d‚Äôimmagine**.  \n",
    "> ‚ùå Offrono una classificazione globale, ma non una segmentazione dettagliata pixel-per-pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fa39e-692d-41f5-ad44-c14625084525",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"artwork/CNN-VGG16.webp\" alt=\"Architettura VGG-16 CNN\" style=\"max-width:100%; height:auto;\">\n",
    "  <figcaption style=\"font-size:90%; color:#444; margin-top:6px;\">\n",
    "    Figura 3. Architettura di una <b>Convolutional Neural Network (CNN)</b>: esempio del modello <b>VGG-16</b>, \n",
    "    caratterizzato da blocchi di convoluzione+ReLU (blu), livelli di max pooling (rosso) e strati fully connected (verde). \n",
    "    L‚Äôinput (224√ó224√ó3) viene progressivamente trasformato fino ad arrivare all‚Äôoutput finale con 1000 classi.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c124ee-3815-4206-b9d0-cd1421a448e8",
   "metadata": {},
   "source": [
    "### 3. U-Net (Segmentazione Semantica)\n",
    "\n",
    "La **U-Net** √® un'architettura avanzata che nasce per applicazioni di segmentazione biomedica ma si √® affermata anche nel telerilevamento.\n",
    "\n",
    "- **Funzionamento**: segue una struttura a \"U\", con una fase di **downsampling (encoder)** per estrarre caratteristiche sempre pi√π astratte, e una fase di **upsampling (decoder)** per ricostruire la mappa segmentata a livello pixel.\n",
    "- **Applicazione**: perfetta per compiti in cui √® importante sapere **esattamente quali pixel appartengono a quale classe**, come nella **mappatura dell‚Äôuso/occupazione del suolo**.\n",
    "- **Vantaggio**: **segmentazione precisa**, sfrutta sia l‚Äôinformazione locale che quella globale.\n",
    "\n",
    "> ‚úÖ Fondamentale per compiti di **segmentazione semantica**.  \n",
    "> ‚ùå Pi√π complessa e richiede dataset pi√π strutturati e costosi da etichettare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f71b8b-751c-49a4-845b-6d0210dba4e6",
   "metadata": {},
   "source": [
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"artwork/2d+Unet_classic.png\" alt=\"Architettura U-Net per segmentazione\" style=\"max-width:100%; height:auto;\">\n",
    "  <figcaption style=\"font-size:90%; color:#444; margin-top:6px;\">\n",
    "    Figura 4. Architettura <b>U-Net</b> per la <b>segmentazione</b>: encoder (sinistra) con blocchi\n",
    "    <i>Conv 3√ó3 + ReLU</i> e <i>Max Pool 2√ó2</i> che comprimono la risoluzione e aumentano i canali; \n",
    "    decoder (destra) con <i>Up-Conv 2√ó2</i> e <i>skip connections</i> (copiatura &amp; crop) che \n",
    "    riutilizzano il dettaglio dell‚Äôencoder; uscita finale tramite <i>Conv 1√ó1</i> che produce la mappa di classe pixel-wise.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6aa252-f5ea-421b-9331-995d7ae3db0f",
   "metadata": {},
   "source": [
    "### Conclusione: perch√© salire di complessit√†?\n",
    "\n",
    "Il passaggio da **ANN ‚Üí CNN ‚Üí U-Net** riflette una crescita nella capacit√† del modello di **comprendere la struttura dell‚Äôimmagine** e **localizzare l‚Äôinformazione rilevante**:\n",
    "\n",
    "- ANN √® un primo approccio, utile per mostrare concetti semplici e lavorare su dati tabellari (pixel ‚Üí vettore di input).\n",
    "- CNN introduce la **consapevolezza spaziale**, fondamentale per trattare immagini in modo coerente.\n",
    "- U-Net risponde all‚Äôesigenza di una **mappatura fine**, dove la posizione e la forma delle classi (es. aree urbane, coltivi, corpi idrici) sono fondamentali.\n",
    "\n",
    "> La scelta del modello dipende quindi **dal tipo di problema** (classificazione globale vs segmentazione dettagliata), **dalla qualit√† e quantit√† dei dati** e dagli **obiettivi dell‚Äôanalisi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8040b-60a9-4f04-9ca2-d9a78619d66f",
   "metadata": {},
   "source": [
    "### Tabella comparativa\n",
    "\n",
    "| Caratteristica                  | ANN / FFNN                    | CNN                              | U-Net                                |\n",
    "|----------------------------------|-------------------------------|-----------------------------------|----------------------------------------|\n",
    "| Struttura spaziale considerata  | ‚ùå No                        | ‚úÖ S√¨ (pattern locali)          | ‚úÖ‚úÖ S√¨ (locale + globale)             |\n",
    "| Tipo di input                   | Pixel indipendenti (vettori) | Immagini o patch                 | Immagini intere (segmentazione)        |\n",
    "| Output                          | Etichetta per pixel singolo   | Etichetta per immagine o patch   | Mappa etichette per pixel             |\n",
    "| Complessit√†                     | üü¢ Bassa                      | üü° Media                         | üî¥ Alta                                |\n",
    "| Requisiti dati (etichettatura) | üü¢ Ridotti                    | üü° Moderati                      | üî¥ Elevati (etichette pixel-level)     |\n",
    "| Adatto per...                   | Classificazione base          | Classificazione immagini         | Segmentazione semantica                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4398e-9f34-4566-b0a8-e1635dba6f87",
   "metadata": {},
   "source": [
    "## Riferimenti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39871ca-53f2-4911-b51d-27a739811f45",
   "metadata": {},
   "source": [
    "### Link utili\n",
    " - [Docker Desktop install](https://docs.docker.com/desktop/)\n",
    " - [Docker compose](https://docs.docker.com/compose/)\n",
    " - [Documentazione TensorFlow](https://www.tensorflow.org/api_docs/python/tf/all_symbols)\n",
    " - [Tutorial TensorFlow](https://www.tensorflow.org/tutorials)\n",
    " - [Keras](https://keras.io)\n",
    " - [Yolo Classification](https://docs.ultralytics.com/tasks/classify/)\n",
    " - Book: [Deep Learning with Python](https://livebook.manning.com/book/deep-learning-with-python-second-edition/meap-version-7/v-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f7e78-621c-4ed6-9f5b-6971b8fad57e",
   "metadata": {},
   "source": [
    "### Caso studio: EuroSAT\n",
    " - [Zenodo](https://doi.org/10.5281/zenodo.7711810)\n",
    " - [Sessione pratica (Jupyter notebook)](00_eSat_custom.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
